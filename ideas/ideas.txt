This file contains ideas that have not yet been implemented in the code.

# Model the chaser as a uniform density cube with 12 thrusters (Gaudet2020d).

# Consider expanding the initial position range in the x and z directions.
# (So the agent might learn the controls in those directions better. Right now its always close to zero)

# Render: Display some sort of warning when the chaser enters the keep-out-zone. Draw a red circle:

    if self.absolute_position() < self.target_radius:
        warning = rendering.make_circle(
            radius=self.target_radius/2,
            res=20,
            filled=True
        )
        warning.set_color(1, 0.3, 0.3)
        self.viewer.add_onetime(warning)

# From SB3 examples:
    - Multiprocessing: use vectorized environments to stack multiple independent environments into a single environment.
    It allows us to train n environments per step, instead of just one. To enable multiprocessing, use SuprocVecEnv instead of DummyVecEnv.
    It allows "significant speed-up when the environment is computationally complex."
    - Dictionary observations: for observations that cannot be directly concatenated. Note that their PPO algorithm does not accept dict actions.
    - Hindsight experience replay (HER): off-policy algorithm? It learns from previous mistakes?
    - Learning rate schedule: Change learning rate based on training completion. It could make the learning more stable.

# Receding bubble reward function:
    Reward function only penalizes fuel consumption.
    Have an imaginary bubble around the target. The bubble decreases in size over time.
    If the chaser exits the bubble, the episode ends. This incentivizes the chaser to go near the target
    to keep the episode going. Penalize collision? Docking bonus?

# Old reward function:
    rew = max(100 - dist, 0) * 1e-2  # range: 0-1

    # Reward for staying close to the corridor axis at all times:
    # rew += (1 - self.angle_from_corridor()/pi) * 1e-1  # range: 0-0.1
    if self.angle_from_corridor() < self.cone_half_angle:
        rew += 0.1

    # Penalize collision:
    if (dist < self.target_radius) & (self.angle_from_corridor() > self.cone_half_angle):
        self.collided = True
        rew -= 0.1
    # rew = (1000 - dist)*1e-2 - np.linalg.norm(action)
    # rew = - np.linalg.norm(action)  # Should the action be part of the state?

    # Done: End the episode if the chaser leaves the observation state or if the max time is reached.
    if ~self.observation_space.contains(obs) or (self.t >= self.t_max):
        done = True
    # Constraint for successful rendezvous:
    elif (dist < 3) and (vel < 0.1) and (self.angle_from_corridor() < self.cone_half_angle) and \
            (self.collided is False):
        rew += 2 * (self.t_max - self.t)
        done = True
    else:
        done = False

# Convert actions from a Discrete(27) space into meaningful delta V actions:
    def convert_discrete_action(self, discrete_action):
        actions = np.array([
            [0, 0, 0],      # 0
            [0, 0, 1],      # U
            [0, 0, -1],     # D
            [-1, 0, 0],     # L
            [1, 0, 0],      # R
            [0, 1, 0],      # F
            [0, -1, 0],     # B
            [-1, 0, 1],     # UL
            [1, 0, 1],      # UR
            [0, 1, 1],      # UF
            [0, -1, 1],     # UB
            [-1, 0, -1],    # DL
            [1, 0, -1],     # DR
            [0, 1, -1],     # DF
            [0, -1, -1],    # DB
            [-1, 1, 0],     # LF
            [-1, -1, 0],    # LB
            [1, 1, 0],      # RF
            [1, -1, 0],     # RB
            [-1, 1, 1],     # ULF
            [-1, -1, 1],    # ULB
            [1, -1, 1],     # URB
            [1, 1, 1],      # URF
            [-1, 1, -1],    # DLF
            [-1, -1, -1],   # DLB
            [1, -1, -1],    # DRB
            [1, 1, -1]      # DRF
        ])
        return actions[discrete_action] * self.max_delta_v

# Get a random sample from the action space, and process it as necessary (based on its type):
    def get_action_sample(self):
        """
        Get a sample from the action space. Works for MultiBinary(6), and MultiDiscrete([3, 3, 3]) action spaces.\n
        :return: action sample
        """
        action = self.action_space.sample()

        if isinstance(self.action_space, gym.spaces.multi_binary.MultiBinary):
            action = (action[0:3] - action[3:]) * self.max_delta_v
        elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):
            action = (action - 1) * self.max_delta_v
        elif isinstance(self.action_space, gym.spaces.box.Box):
            action = action
        else:
            raise TypeError(f'Action space type is incompatible: {type(self.action_space)}')

        return action


# 6-DOF old reward function:
# def old_get_reward(self):
    #     """
    #     This function is not used at the moment. Using get_bubble_reward() instead.
    #     Calculate the reward for the current timestep.
    #     Maybe use a quadratic penalty for velocity and control effort.\n
    #     :return: reward
    #     """
    #
    #     assert self.rc is not None
    #
    #     goal_pos = np.matmul(quat2mat(self.qt), self.rd)    # Goal position in the LVLH frame [m]
    #     goal_vel = np.cross(self.wt, goal_pos)              # Goal velocity in the LVLH frame [m/s]
    #     pos_error = np.linalg.norm(self.rc - goal_pos)      # Position error [m]
    #     vel_error = np.linalg.norm(self.vc - goal_vel)      # Velocity error [m/s]
    #     att_error = self.attitude_error()                   # Attitude error [rad]
    #     rot_error = np.linalg.norm(self.wc - self.wt)       # Rotation rate error [rad/s]
    #
    #     rew = 0
    #
    #     # Coefficients:
    #     c_r = 2     # position-based reward coefficient
    #     c_v = 0     # velocity-based reward coefficient
    #     c_q = 1     # attitude-based reward coefficient
    #     c_w = 0     # rotation-based reward coefficient
    #
    #     if pos_error < 10:
    #         reverse_sigmoid = 1 / (1 + np.e**(5 * pos_error - 10))
    #     else:
    #         reverse_sigmoid = 0  # Prevent runtime warnings when e^pos_error becomes too large
    #
    #     rew += c_r * (1 - pos_error / self.max_axial_distance)
    #     rew += c_v * (1 - vel_error / self.max_axial_speed) * reverse_sigmoid
    #     rew += c_q * (1 - att_error / np.pi)
    #     rew += c_w * (1 - rot_error / self.max_rotation_rate) * reverse_sigmoid
    #
    #     # Distance-based reward:
    #     # rew += (self.max_axial_distance - np.linalg.norm(pos_error)) / self.max_axial_distance
    #     # Attitude-based reward:
    #     # rew += 1e-1 * (np.pi - att_error) / np.pi
    #
    #     # Success bonus:
    #     errors = np.array([
    #         pos_error,
    #         vel_error,
    #         att_error,
    #         rot_error,
    #     ])
    #
    #     ranges = np.array([
    #         self.max_rd_error,
    #         self.max_vd_error,
    #         self.max_qd_error,
    #         self.max_wd_error,
    #     ])
    #
    #     if not self.collided:
    #         if np.all(errors < ranges):
    #             rew += 1000
    #         else:
    #             threshold = self.max_rd_error * 10
    #             if pos_error < threshold:
    #                 rew += 10 * (1 - pos_error / threshold)
    #
    #     # SciPy method:
    #     # rot = scipyRot.from_quat(put_scalar_last(self.qc))
    #     # chaser_neg_y = rot.apply(np.array([0, -1, 0]))
    #     # print(f'reward: {rew}')
    #
    #     return rew

# Old action-processing functions:
    def process_box(self, action):
        """
        Process an action taken from a Box action space. We need to convert the action into a change in velocity and a
        change in rotation rate of the chaser. The delta_v needs to be expressed in the LVLH frame, but the delta_w can
        remain in the chaser body frame.\n
        :param action: action sampled from a Box action space.
        :return: change in velocity and change rotation rate of the chaser
        """

        # delta_v = action[0:3] * self.max_delta_v
        delta_v = np.matmul(quat2mat(self.qc), action[0:3] * self.max_delta_v)  # rotated to LVLH frame
        delta_w = action[3:] * self.max_delta_w  # already expressed in the body frame (no rotation needed)

        return delta_v, delta_w

    def process_multidiscrete(self, action):
        """
        Process an action taken from a MultiDiscrete action space. In addition to the same processing used in
        process_box(), we need to subtract 1 because the output of the action space is 0, 1, or 2.\n
        :param action: action sampled from a MultiDiscrete action space.
        :return: change in velocity and change in rotation rate of the chaser
        """

        delta_v, delta_w = self.process_box(action-1)

        return delta_v, delta_w

# Different way to rotate body points: (does not work for KOZ)
    points = np.vstack((x, y, z)).T
    print(points.shape)
    new_points = rot.apply(points)
    new_x, new_y, new_z = new_points.T