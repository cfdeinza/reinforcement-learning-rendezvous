This file contains ideas that have not yet been implemented in the code.

# Consider expanding the initial position range in the x and z directions.
# (So the agent might learn the controls in those directions better. Right now its always close to zero)

# Render: Display some sort of warning when the chaser enters the keep-out-zone. Draw a red circle:

    if self.absolute_position() < self.target_radius:
        warning = rendering.make_circle(
            radius=self.target_radius/2,
            res=20,
            filled=True
        )
        warning.set_color(1, 0.3, 0.3)
        self.viewer.add_onetime(warning)

# From SB3 examples:
    - Multiprocessing: use vectorized environments to stack multiple independent environments into a single environment.
    It allows us to train n environments per step, instead of just one. To enable multiprocessing, use SuprocVecEnv instead of DummyVecEnv.
    It allows "significant speed-up when the environment is computationally complex."
    - Dictionary observations: for observations that cannot be directly concatenated. Note that their PPO algorithm does not accept dict actions.
    - Hindsight experience replay (HER): off-policy algorithm? It learns from previous mistakes?
    - Learning rate schedule: Change learning rate based on training completion. It could make the learning more stable.

# Receding bubble reward function:
    Reward function only penalizes fuel consumption.
    Have an imaginary bubble around the target. The bubble decreases in size over time.
    If the chaser exits the bubble, the episode ends. This incentivizes the chaser to go near the target
    to keep the episode going. Penalize collision? Docking bonus?

# Old reward function:
    rew = max(100 - dist, 0) * 1e-2  # range: 0-1

    # Reward for staying close to the corridor axis at all times:
    # rew += (1 - self.angle_from_corridor()/pi) * 1e-1  # range: 0-0.1
    if self.angle_from_corridor() < self.cone_half_angle:
        rew += 0.1

    # Penalize collision:
    if (dist < self.target_radius) & (self.angle_from_corridor() > self.cone_half_angle):
        self.collided = True
        rew -= 0.1
    # rew = (1000 - dist)*1e-2 - np.linalg.norm(action)
    # rew = - np.linalg.norm(action)  # Should the action be part of the state?

    # Done: End the episode if the chaser leaves the observation state or if the max time is reached.
    if ~self.observation_space.contains(obs) or (self.t >= self.t_max):
        done = True
    # Constraint for successful rendezvous:
    elif (dist < 3) and (vel < 0.1) and (self.angle_from_corridor() < self.cone_half_angle) and \
            (self.collided is False):
        rew += 2 * (self.t_max - self.t)
        done = True
    else:
        done = False

# Convert actions from a Discrete(27) space into meaningful delta V actions:
    def convert_discrete_action(self, discrete_action):
        actions = np.array([
            [0, 0, 0],      # 0
            [0, 0, 1],      # U
            [0, 0, -1],     # D
            [-1, 0, 0],     # L
            [1, 0, 0],      # R
            [0, 1, 0],      # F
            [0, -1, 0],     # B
            [-1, 0, 1],     # UL
            [1, 0, 1],      # UR
            [0, 1, 1],      # UF
            [0, -1, 1],     # UB
            [-1, 0, -1],    # DL
            [1, 0, -1],     # DR
            [0, 1, -1],     # DF
            [0, -1, -1],    # DB
            [-1, 1, 0],     # LF
            [-1, -1, 0],    # LB
            [1, 1, 0],      # RF
            [1, -1, 0],     # RB
            [-1, 1, 1],     # ULF
            [-1, -1, 1],    # ULB
            [1, -1, 1],     # URB
            [1, 1, 1],      # URF
            [-1, 1, -1],    # DLF
            [-1, -1, -1],   # DLB
            [1, -1, -1],    # DRB
            [1, 1, -1]      # DRF
        ])
        return actions[discrete_action] * self.max_delta_v

# Get a random sample from the action space, and process it as necessary (based on its type):
    def get_action_sample(self):
        """
        Get a sample from the action space. Works for MultiBinary(6), and MultiDiscrete([3, 3, 3]) action spaces.\n
        :return: action sample
        """
        action = self.action_space.sample()

        if isinstance(self.action_space, gym.spaces.multi_binary.MultiBinary):
            action = (action[0:3] - action[3:]) * self.max_delta_v
        elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):
            action = (action - 1) * self.max_delta_v
        elif isinstance(self.action_space, gym.spaces.box.Box):
            action = action
        else:
            raise TypeError(f'Action space type is incompatible: {type(self.action_space)}')

        return action


# Different way to rotate body points: (does not work for KOZ)
    points = np.vstack((x, y, z)).T
    print(points.shape)
    new_points = rot.apply(points)
    new_x, new_y, new_z = new_points.T