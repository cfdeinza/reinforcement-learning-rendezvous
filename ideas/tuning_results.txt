Best hyperparameters: 
(based on alg_sweep_02, performed ~100 runs on MLP model with 50 iterations of 2048 steps -> 100k timesteps)

batch_size: irrelevant (as long as it is below n_steps, to avoid local optima)
clip_range:[0.2 - 0.3], try using 0.25
learning_rate: [7e-4 - 2e-2], try using 2e-3
n_epochs: [33 - 85], try using 60

batch_size and n_epochs determine how long it takes to train the model. Try to stick to higher batch_size and 
lower n_epochs.

Params which work well for RNN: (used to train the "decent" models)
n_steps = 4096
batch_size = 128
learning_rate = 5e-4 (switched to 5e-5 after a while)
clip_range = 0.2
n_epochs = 35
gamma = 0.99
