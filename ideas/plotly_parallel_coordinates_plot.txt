How to create a parallel coordinates plot using plotly:

import numpy as np
import plotly.graph_objects as go

num = 10
successes = np.random.random(size=(num,))
rc0_mag = np.random.random(size=(num,))
vc0_mag = np.random.random(size=(num,))
qc0_mag = np.random.random(size=(num,))
wc0_mag = np.random.random(size=(num,))
qt0_mag = np.random.random(size=(num,))
wt0_mag = np.random.random(size=(num,))

fig = go.Figure(
        data=go.Parcoords(
            line=dict(
                color=successes,
                colorscale="Viridis",
                showscale=True,
            ),
            dimensions=list([
                dict(
                    range=[9, 11],
                    # constraintrange=[4, 8],
                    label="r_c",
                    values=rc0_mag,
                ),
                dict(
                    range=[0, 0.1],
                    label="v_c",
                    values=vc0_mag,
                ),
                dict(
                    range=[0, 1],
                    label="q_c",
                    values=qc0_mag,
                ),
                dict(
                    range=[0, 0.1],
                    label="w_c",
                    values=wc0_mag,
                ),
                dict(
                    range=[0, 45],
                    label="q_t",
                    values=qt0_mag,
                ),
                dict(
                    range=[0, 3],
                    label="w_t",
                    values=wt0_mag,
                ),
                dict(
                    range=[0, max(successes) + 1],
                    label="Success",
                    values=successes,
                )
            ]),
        )
    )

    fig.show()


# rc0 = []
# vc0 = []
# qc0 = []
# wc0 = []
# qt0 = []
# wt0 = []
# pos_errors = []
# vel_errors = []
# att_errors = []
# rot_errors = []
# successes = []
# for i in range(n_evals):
#     print(f"Eval: {i + 1}/{n_evals}")
#     output = evaluate_episode(saved_model, eval_env)
#     rc0.append(output["rc0"])
#     vc0.append(output["vc0"])
#     qc0.append(output["qc0"])
#     wc0.append(output["wc0"])
#     qt0.append(output["qt0"])
#     wt0.append(output["wt0"])
#     pos_errors.append(output["errors"][0])
#     vel_errors.append(output["errors"][1])
#     att_errors.append(output["errors"][2])
#     rot_errors.append(output["errors"][3])
#     successes.append(output["successes"])
#
# rc0 = np.vstack(rc0).T
# vc0 = np.vstack(vc0).T
# qc0 = np.vstack(qc0).T
# wc0 = np.vstack(wc0).T
# qt0 = np.vstack(qt0).T
# wt0 = np.vstack(wt0).T
#
# # Compute magnitude of initial states:
# rc0_mag = np.linalg.norm(rc0, axis=0)
# vc0_mag = np.linalg.norm(vc0, axis=0)
# qc0_mag = np.degrees(2 * np.arccos(qc0[0]))
# wc0_mag = np.degrees(np.linalg.norm(wc0, axis=0))
# qt0_mag = np.degrees(2 * np.arccos(qt0[0]))
# wt0_mag = np.degrees(np.linalg.norm(wt0, axis=0))

# def evaluate_episode(model, env):
#
#     # Create empty arrays to store results:
#     expected_timesteps = int(env.t_max / env.dt) + 1  # size of the empty arrays
#     errors = np.full((4, expected_timesteps), np.nan)  # array where errors will be saved
#     t = np.full((1, expected_timesteps), np.nan)
#
#     # Reset environment:
#     obs = env.reset()
#     lstm_states = None
#     ep_start = np.ones(shape=(1,), dtype=bool)
#     done = False
#     errors[:, 0] = env.get_errors()
#     t[0, 0] = env.t
#     collisions = int(env.check_collision())
#
#     # Record the initial state:
#     out = dict(
#         rc0=env.rc,
#         vc0=env.vc,
#         qc0=env.qc,
#         wc0=env.wc,
#         qt0=env.qt,
#         wt0=env.wt,
#     )
#
#     k = 1
#     while not done:
#         # Select action:
#         action, lstm_states = model.predict(
#             observation=obs,  # input to the policy network
#             state=lstm_states,  # last hidden state (used for recurrent policies)
#             episode_start=ep_start,  # the last mask? (used for recurrent policies)
#             deterministic=True  # whether or not to return deterministic actions (default is False)
#         )
#
#         # Step forward in time:
#         obs, reward, done, info = env.step(action)
#         ep_start[0] = done
#
#         # Update data:
#         errors[:, k] = env.get_errors()
#         t[0, k] = env.t
#         collisions += int(env.check_collision())
#         k += 1
#
#     out["errors"] = errors
#     out["t"] = t
#     out["collisions"] = collisions
#     out["successes"] = env.success
#
#     return out